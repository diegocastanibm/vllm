# VLLM_USE_V1=1 VLLM_LOGGING_LEVEL=DEBUG vllm serve meta-llama/Llama-3.1-8B-Instruct
DEBUG 07-03 03:54:51 [__init__.py:31] No plugins for group vllm.platform_plugins found.
DEBUG 07-03 03:54:51 [__init__.py:35] Checking if TPU platform is available.
DEBUG 07-03 03:54:51 [__init__.py:45] TPU platform is not available because: No module named 'libtpu'
DEBUG 07-03 03:54:51 [__init__.py:52] Checking if CUDA platform is available.
DEBUG 07-03 03:54:51 [__init__.py:72] Confirmed CUDA platform is available.
DEBUG 07-03 03:54:51 [__init__.py:100] Checking if ROCm platform is available.
DEBUG 07-03 03:54:51 [__init__.py:114] ROCm platform is not available because: No module named 'amdsmi'
DEBUG 07-03 03:54:51 [__init__.py:121] Checking if HPU platform is available.
DEBUG 07-03 03:54:51 [__init__.py:128] HPU platform is not available because habana_frameworks is not found.
DEBUG 07-03 03:54:51 [__init__.py:138] Checking if XPU platform is available.
DEBUG 07-03 03:54:51 [__init__.py:148] XPU platform is not available because: No module named 'intel_extension_for_pytorch'
DEBUG 07-03 03:54:51 [__init__.py:155] Checking if CPU platform is available.
DEBUG 07-03 03:54:51 [__init__.py:177] Checking if Neuron platform is available.
DEBUG 07-03 03:54:51 [__init__.py:52] Checking if CUDA platform is available.
DEBUG 07-03 03:54:51 [__init__.py:72] Confirmed CUDA platform is available.
INFO 07-03 03:54:51 [__init__.py:244] Automatically detected platform cuda.
DEBUG 07-03 03:54:54 [utils.py:150] Setting VLLM_WORKER_MULTIPROC_METHOD to 'spawn'
DEBUG 07-03 03:54:54 [__init__.py:39] Available plugins for group vllm.general_plugins:
DEBUG 07-03 03:54:54 [__init__.py:41] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
DEBUG 07-03 03:54:54 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 07-03 03:54:54 [api_server.py:1388] vLLM API server version 0.1.dev7329+g04e1642
INFO 07-03 03:54:54 [cli_args.py:314] non-default args: {'model': 'meta-llama/Llama-3.1-8B-Instruct'}
INFO 07-03 03:55:01 [config.py:853] This model supports multiple tasks: {'classify', 'reward', 'generate', 'score', 'embed'}. Defaulting to 'generate'.
INFO 07-03 03:55:01 [config.py:1467] Using max model len 131072
DEBUG 07-03 03:55:01 [arg_utils.py:1676] Setting max_num_batched_tokens to 2048 for OPENAI_API_SERVER usage context.
DEBUG 07-03 03:55:01 [arg_utils.py:1684] Setting max_num_seqs to 256 for OPENAI_API_SERVER usage context.
INFO 07-03 03:55:01 [config.py:2267] Chunked prefill is enabled with max_num_batched_tokens=2048.
DEBUG 07-03 03:55:05 [__init__.py:31] No plugins for group vllm.platform_plugins found.
DEBUG 07-03 03:55:05 [__init__.py:35] Checking if TPU platform is available.
DEBUG 07-03 03:55:05 [__init__.py:45] TPU platform is not available because: No module named 'libtpu'
DEBUG 07-03 03:55:05 [__init__.py:52] Checking if CUDA platform is available.
DEBUG 07-03 03:55:05 [__init__.py:72] Confirmed CUDA platform is available.
DEBUG 07-03 03:55:05 [__init__.py:100] Checking if ROCm platform is available.
DEBUG 07-03 03:55:05 [__init__.py:114] ROCm platform is not available because: No module named 'amdsmi'
DEBUG 07-03 03:55:05 [__init__.py:121] Checking if HPU platform is available.
DEBUG 07-03 03:55:05 [__init__.py:128] HPU platform is not available because habana_frameworks is not found.
DEBUG 07-03 03:55:05 [__init__.py:138] Checking if XPU platform is available.
DEBUG 07-03 03:55:05 [__init__.py:148] XPU platform is not available because: No module named 'intel_extension_for_pytorch'
DEBUG 07-03 03:55:05 [__init__.py:155] Checking if CPU platform is available.
DEBUG 07-03 03:55:05 [__init__.py:177] Checking if Neuron platform is available.
DEBUG 07-03 03:55:05 [__init__.py:52] Checking if CUDA platform is available.
DEBUG 07-03 03:55:05 [__init__.py:72] Confirmed CUDA platform is available.
INFO 07-03 03:55:05 [__init__.py:244] Automatically detected platform cuda.
INFO 07-03 03:55:08 [core.py:459] Waiting for init message from front-end.
DEBUG 07-03 03:55:08 [utils.py:547] HELLO from local core engine process 0.
DEBUG 07-03 03:55:08 [core.py:467] Received init message: EngineHandshakeMetadata(addresses=EngineZmqAddresses(inputs=['ipc:///tmp/aa1a4bfb-928c-4f93-bacd-74b3d653c179'], outputs=['ipc:///tmp/47c787b8-809a-4502-992b-8da3cb29d14b'], coordinator_input=None, coordinator_output=None), parallel_config={'data_parallel_master_ip': '127.0.0.1', 'data_parallel_master_port': 0, 'data_parallel_size': 1})
DEBUG 07-03 03:55:08 [__init__.py:39] Available plugins for group vllm.general_plugins:
DEBUG 07-03 03:55:08 [__init__.py:41] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
DEBUG 07-03 03:55:08 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 07-03 03:55:08 [core.py:69] Initializing a V1 LLM engine (v0.1.dev7329+g04e1642) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
DEBUG 07-03 03:55:08 [decorators.py:110] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.llama.LlamaModel'>: ['input_ids', 'positions', 'intermediate_tensors', 'inputs_embeds']
DEBUG 07-03 03:55:08 [decorators.py:110] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.llama_eagle3.LlamaModel'>: ['input_ids', 'positions', 'hidden_states']
WARNING 07-03 03:55:08 [utils.py:2753] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7fbd35b236e0>
DEBUG 07-03 03:55:08 [config.py:4806] enabled custom ops: Counter()
DEBUG 07-03 03:55:08 [config.py:4808] disabled custom ops: Counter()
DEBUG 07-03 03:55:09 [parallel_state.py:919] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.129.4.27:40985 backend=nccl
DEBUG 07-03 03:55:09 [parallel_state.py:970] Detected 1 nodes in the distributed environment
INFO 07-03 03:55:09 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 07-03 03:55:09 [topk_topp_sampler.py:49] Using FlashInfer for top-p & top-k sampling.
DEBUG 07-03 03:55:09 [config.py:4806] enabled custom ops: Counter()
DEBUG 07-03 03:55:09 [config.py:4808] disabled custom ops: Counter()
INFO 07-03 03:55:09 [gpu_model_runner.py:1724] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 07-03 03:55:09 [gpu_model_runner.py:1729] Loading model from scratch...
INFO 07-03 03:55:09 [cuda.py:270] Using Flash Attention backend on V1 engine.
DEBUG 07-03 03:55:09 [backends.py:39] Using InductorAdaptor
DEBUG 07-03 03:55:09 [config.py:4806] enabled custom ops: Counter()
DEBUG 07-03 03:55:09 [config.py:4808] disabled custom ops: Counter({'rms_norm': 65, 'silu_and_mul': 32, 'rotary_embedding': 1})
INFO 07-03 03:55:09 [weight_utils.py:292] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:02,  1.20it/s]
DEBUG 07-03 03:55:18 [utils.py:485] Waiting for 1 local, 0 remote core engine proc(s) to start.
DEBUG 07-03 03:55:28 [utils.py:485] Waiting for 1 local, 0 remote core engine proc(s) to start.
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:20<00:24, 12.14s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:21<00:07,  7.05s/it]
DEBUG 07-03 03:55:31 [utils.py:170] Loaded weight lm_head.weight with shape torch.Size([128256, 4096])
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:22<00:00,  4.40s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:22<00:00,  5.55s/it]

INFO 07-03 03:55:31 [default_loader.py:272] Loading weights took 22.24 seconds
INFO 07-03 03:55:32 [gpu_model_runner.py:1755] Model loading took 14.9889 GiB and 22.527575 seconds
DEBUG 07-03 03:55:32 [decorators.py:204] Start compiling function <code object forward at 0xc7e4960, file "/app/vllm/vllm/model_executor/models/llama.py", line 368>
DEBUG 07-03 03:55:38 [utils.py:485] Waiting for 1 local, 0 remote core engine proc(s) to start.
DEBUG 07-03 03:55:38 [backends.py:466] Traced files (to be considered for compilation cache):
DEBUG 07-03 03:55:38 [backends.py:466] /app/vllm/vllm/attention/layer.py
DEBUG 07-03 03:55:38 [backends.py:466] /app/vllm/vllm/distributed/communication_op.py
DEBUG 07-03 03:55:38 [backends.py:466] /app/vllm/vllm/distributed/parallel_state.py
DEBUG 07-03 03:55:38 [backends.py:466] /app/vllm/vllm/model_executor/custom_op.py
DEBUG 07-03 03:55:38 [backends.py:466] /app/vllm/vllm/model_executor/layers/activation.py
DEBUG 07-03 03:55:38 [backends.py:466] /app/vllm/vllm/model_executor/layers/layernorm.py
DEBUG 07-03 03:55:38 [backends.py:466] /app/vllm/vllm/model_executor/layers/linear.py
DEBUG 07-03 03:55:38 [backends.py:466] /app/vllm/vllm/model_executor/layers/rotary_embedding.py
DEBUG 07-03 03:55:38 [backends.py:466] /app/vllm/vllm/model_executor/layers/utils.py
DEBUG 07-03 03:55:38 [backends.py:466] /app/vllm/vllm/model_executor/layers/vocab_parallel_embedding.py
DEBUG 07-03 03:55:38 [backends.py:466] /app/vllm/vllm/model_executor/models/llama.py
DEBUG 07-03 03:55:38 [backends.py:466] /app/vllm/vllm/platforms/interface.py
DEBUG 07-03 03:55:38 [backends.py:466] /usr/local/lib/python3.12/dist-packages/torch/_dynamo/polyfills/__init__.py
DEBUG 07-03 03:55:38 [backends.py:466] /usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py
DEBUG 07-03 03:55:38 [backends.py:466] /usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py
INFO 07-03 03:55:39 [backends.py:513] Using cache directory: /root/.cache/vllm/torch_compile_cache/56d5ca6229/rank_0_0/backbone for vLLM's torch.compile
INFO 07-03 03:55:39 [backends.py:524] Dynamo bytecode transform time: 6.70 s
DEBUG 07-03 03:55:39 [backends.py:125] Loading time: 0.051212	 Directly load the 0-th graph for shape None from inductor via handle ('f4hbzzek7ars4k72fosn7f4mnkkwptvti72chcqqebgki5i5v5t7', '/root/.cache/vllm/torch_compile_cache/56d5ca6229/rank_0_0/inductor_cache/52/c52ugwbtzppi2gzpmmddk5ucou72iwptj3cm23ymruc464cbmn4s.py')
DEBUG 07-03 03:55:39 [backends.py:153] TOTAL LOADING TIME: 0.051351 s
DEBUG 07-03 03:55:39 [backends.py:125] Loading time: 0.053071	 Directly load the 1-th graph for shape None from inductor via handle ('fsx6a7bzzfqjfn5ohxknaxvfze4hskr5o2nk4pj5gennyl7u2v3q', '/root/.cache/vllm/torch_compile_cache/56d5ca6229/rank_0_0/inductor_cache/gx/cgxorvo2ds2xu4ghn7wpbcvg4pj2wwwjecyz253ewoycnmi22elm.py')
DEBUG 07-03 03:55:39 [backends.py:153] TOTAL LOADING TIME: 0.053279 s
DEBUG 07-03 03:55:39 [backends.py:125] Loading time: 0.047995	 Directly load the 2-th graph for shape None from inductor via handle ('fsx6a7bzzfqjfn5ohxknaxvfze4hskr5o2nk4pj5gennyl7u2v3q', '/root/.cache/vllm/torch_compile_cache/56d5ca6229/rank_0_0/inductor_cache/gx/cgxorvo2ds2xu4ghn7wpbcvg4pj2wwwjecyz253ewoycnmi22elm.py')
DEBUG 07-03 03:55:39 [backends.py:153] TOTAL LOADING TIME: 0.048108 s
DEBUG 07-03 03:55:39 [backends.py:125] Loading time: 0.052948	 Directly load the 3-th graph for shape None from inductor via handle ('fsx6a7bzzfqjfn5ohxknaxvfze4hskr5o2nk4pj5gennyl7u2v3q', '/root/.cache/vllm/torch_compile_cache/56d5ca6229/rank_0_0/inductor_cache/gx/cgxorvo2ds2xu4ghn7wpbcvg4pj2wwwjecyz253ewoycnmi22elm.py')
DEBUG 07-03 03:55:39 [backends.py:153] TOTAL LOADING TIME: 0.053071 s
DEBUG 07-03 03:55:40 [backends.py:125] Loading time: 0.049760	 Directly load the 4-th graph for shape None from inductor via handle ('fsx6a7bzzfqjfn5ohxknaxvfze4hskr5o2nk4pj5gennyl7u2v3q', '/root/.cache/vllm/torch_compile_cache/56d5ca6229/rank_0_0/inductor_cache/gx/cgxorvo2ds2xu4ghn7wpbcvg4pj2wwwjecyz253ewoycnmi22elm.py')
DEBUG 07-03 03:55:40 [backends.py:153] TOTAL LOADING TIME: 0.049869 s
DEBUG 07-03 03:55:40 [backends.py:125] Loading time: 0.050803	 Directly load the 5-th graph for shape None from inductor via handle ('fsx6a7bzzfqjfn5ohxknaxvfze4hskr5o2nk4pj5gennyl7u2v3q', '/root/.cache/vllm/torch_compile_cache/56d5ca6229/rank_0_0/inductor_cache/gx/cgxorvo2ds2xu4ghn7wpbcvg4pj2wwwjecyz253ewoycnmi22elm.py')
DEBUG 07-03 03:55:40 [backends.py:153] TOTAL LOADING TIME: 0.050916 s
DEBUG 07-03 03:55:40 [backends.py:125] Loading time: 0.053857	 Directly load the 6-th graph for shape None from inductor via handle ('fsx6a7bzzfqjfn5ohxknaxvfze4hskr5o2nk4pj5gennyl7u2v3q', '/root/.cache/vllm/torch_compile_cache/56d5ca6229/rank_0_0/inductor_cache/gx/cgxorvo2ds2xu4ghn7wpbcvg4pj2wwwjecyz253ewoycnmi22elm.py')
DEBUG 07-03 03:55:40 [backends.py:153] TOTAL LOADING TIME: 0.053997 s
DEBUG 07-03 03:55:40 [backends.py:125] Loading time: 0.048016	 Directly load the 7-th graph for shape None from inductor via handle ('fsx6a7bzzfqjfn5ohxknaxvfze4hskr5o2nk4pj5gennyl7u2v3q', '/root/.cache/vllm/torch_compile_cache/56d5ca6229/rank_0_0/inductor_cache/gx/cgxorvo2ds2xu4ghn7wpbcvg4pj2wwwjecyz253ewoycnmi22elm.py')
DEBUG 07-03 03:55:40 [backends.py:153] TOTAL LOADING TIME: 0.048146 s
DEBUG 07-03 03:55:40 [backends.py:125] Loading time: 0.051341	 Directly load the 8-th graph for shape None from inductor via handle ('fsx6a7bzzfqjfn5ohxknaxvfze4hskr5o2nk4pj5gennyl7u2v3q', '/root/.cache/vllm/torch_compile_cache/56d5ca6229/rank_0_0/inductor_cache/gx/cgxorvo2ds2xu4ghn7wpbcvg4pj2wwwjecyz253ewoycnmi22elm.py')
DEBUG 07-03 03:55:40 [backends.py:153] TOTAL LOADING TIME: 0.051450 s
DEBUG 07-03 03:55:40 [backends.py:125] Loading time: 0.050650	 Directly load the 9-th graph for shape None from inductor via handle ('fsx6a7bzzfqjfn5ohxknaxvfze4hskr5o2nk4pj5gennyl7u2v3q', '/root/.cache/vllm/torch_compile_cache/56d5ca6229/rank_0_0/inductor_cache/gx/cgxorvo2ds2xu4ghn7wpbcvg4pj2wwwjecyz253ewoycnmi22elm.py')
DEBUG 07-03 03:55:40 [backends.py:153] TOTAL LOADING TIME: 0.050758 s
DEBUG 07-03 03:55:41 [backends.py:125] Loading time: 0.054216	 Directly load the 10-th graph for shape None from inductor via handle ('fsx6a7bzzfqjfn5ohxknaxvfze4hskr5o2nk4pj5gennyl7u2v3q', '/root/.cache/vllm/torch_compile_cache/56d5ca6229/rank_0_0/inductor_cache/gx/cgxorvo2ds2xu4ghn7wpbcvg4pj2wwwjecyz253ewoycnmi22elm.py')
DEBUG 07-03 03:55:41 [backends.py:153] TOTAL LOADING TIME: 0.054330 s
DEBUG 07-03 03:55:41 [backends.py:125] Loading time: 0.050273	 Directly load the 11-th graph for shape None from inductor via handle ('fsx6a7bzzfqjfn5ohxknaxvfze4hskr5o2nk4pj5gennyl7u2v3q', '/root/.cache/vllm/torch_compile_cache/56d5ca6229/rank_0_0/inductor_cache/gx/cgxorvo2ds2xu4ghn7wpbcvg4pj2wwwjecyz253ewoycnmi22elm.py')
DEBUG 07-03 03:55:41 [backends.py:153] TOTAL LOADING TIME: 0.050382 s
DEBUG 07-03 03:55:41 [backends.py:125] Loading time: 0.053713	 Directly load the 12-th graph for shape None from inductor via handle ('fsx6a7bzzfqjfn5ohxknaxvfze4hskr5o2nk4pj5gennyl7u2v3q', '/root/.cache/vllm/torch_compile_cache/56d5ca6229/rank_0_0/inductor_cache/gx/cgxorvo2ds2xu4ghn7wpbcvg4pj2wwwjecyz253ewoycnmi22elm.py')
DEBUG 07-03 03:55:41 [backends.py:153] TOTAL LOADING TIME: 0.053860 s
DEBUG 07-03 03:55:41 [backends.py:125] Loading time: 0.059119	 Directly load the 13-th graph for shape None from inductor via handle ('fsx6a7bzzfqjfn5ohxknaxvfze4hskr5o2nk4pj5gennyl7u2v3q', '/root/.cache/vllm/torch_compile_cache/56d5ca6229/rank_0_0/inductor_cache/gx/cgxorvo2ds2xu4ghn7wpbcvg4pj2wwwjecyz253ewoycnmi22elm.py')
DEBUG 07-03 03:55:41 [backends.py:153] TOTAL LOADING TIME: 0.059233 s
DEBUG 07-03 03:55:41 [backends.py:125] Loading time: 0.050212	 Directly load the 14-th graph for shape None from inductor via handle ('fsx6a7bzzfqjfn5ohxknaxvfze4hskr5o2nk4pj5gennyl7u2v3q', '/root/.cache/vllm/torch_compile_cache/56d5ca6229/rank_0_0/inductor_cache/gx/cgxorvo2ds2xu4ghn7wpbcvg4pj2wwwjecyz253ewoycnmi22elm.py')
DEBUG 07-03 03:55:41 [backends.py:153] TOTAL LOADING TIME: 0.050339 s
DEBUG 07-03 03:55:41 [backends.py:125] Loading time: 0.056365	 Directly load the 15-th graph for shape None from inductor via handle ('fsx6a7bzzfqjfn5ohxknaxvfze4hskr5o2nk4pj5gennyl7u2v3q', '/root/.cache/vllm/torch_compile_cache/56d5ca6229/rank_0_0/inductor_cache/gx/cgxorvo2ds2xu4ghn7wpbcvg4pj2wwwjecyz253ewoycnmi22elm.py')
DEBUG 07-03 03:55:41 [backends.py:153] TOTAL LOADING TIME: 0.056497 s
DEBUG 07-03 03:55:42 [backends.py:125] Loading time: 0.055686	 Directly load the 16-th graph for shape None from inductor via handle ('fsx6a7bzzfqjfn5ohxknaxvfze4hskr5o2nk4pj5gennyl7u2v3q', '/root/.cache/vllm/torch_compile_cache/56d5ca6229/rank_0_0/inductor_cache/gx/cgxorvo2ds2xu4ghn7wpbcvg4pj2wwwjecyz253ewoycnmi22elm.py')
DEBUG 07-03 03:55:42 [backends.py:153] TOTAL LOADING TIME: 0.055829 s
DEBUG 07-03 03:55:42 [backends.py:125] Loading time: 0.049815	 Directly load the 17-th graph for shape None from inductor via handle ('fsx6a7bzzfqjfn5ohxknaxvfze4hskr5o2nk4pj5gennyl7u2v3q', '/root/.cache/vllm/torch_compile_cache/56d5ca6229/rank_0_0/inductor_cache/gx/cgxorvo2ds2xu4ghn7wpbcvg4pj2wwwjecyz253ewoycnmi22elm.py')
DEBUG 07-03 03:55:42 [backends.py:153] TOTAL LOADING TIME: 0.049929 s
DEBUG 07-03 03:55:42 [backends.py:125] Loading time: 0.055873	 Directly load the 18-th graph for shape None from inductor via handle ('fsx6a7bzzfqjfn5ohxknaxvfze4hskr5o2nk4pj5gennyl7u2v3q', '/root/.cache/vllm/torch_compile_cache/56d5ca6229/rank_0_0/inductor_cache/gx/cgxorvo2ds2xu4ghn7wpbcvg4pj2wwwjecyz253ewoycnmi22elm.py')
DEBUG 07-03 03:55:42 [backends.py:153] TOTAL LOADING TIME: 0.055982 s
DEBUG 07-03 03:55:42 [backends.py:125] Loading time: 0.052412	 Directly load the 19-th graph for shape None from inductor via handle ('fsx6a7bzzfqjfn5ohxknaxvfze4hskr5o2nk4pj5gennyl7u2v3q', '/root/.cache/vllm/torch_compile_cache/56d5ca6229/rank_0_0/inductor_cache/gx/cgxorvo2ds2xu4ghn7wpbcvg4pj2wwwjecyz253ewoycnmi22elm.py')
DEBUG 07-03 03:55:42 [backends.py:153] TOTAL LOADING TIME: 0.052607 s
DEBUG 07-03 03:55:42 [backends.py:125] Loading time: 0.052936	 Directly load the 20-th graph for shape None from inductor via handle ('fsx6a7bzzfqjfn5ohxknaxvfze4hskr5o2nk4pj5gennyl7u2v3q', '/root/.cache/vllm/torch_compile_cache/56d5ca6229/rank_0_0/inductor_cache/gx/cgxorvo2ds2xu4ghn7wpbcvg4pj2wwwjecyz253ewoycnmi22elm.py')
DEBUG 07-03 03:55:42 [backends.py:153] TOTAL LOADING TIME: 0.053042 s
DEBUG 07-03 03:55:42 [backends.py:125] Loading time: 0.051793	 Directly load the 21-th graph for shape None from inductor via handle ('fsx6a7bzzfqjfn5ohxknaxvfze4hskr5o2nk4pj5gennyl7u2v3q', '/root/.cache/vllm/torch_compile_cache/56d5ca6229/rank_0_0/inductor_cache/gx/cgxorvo2ds2xu4ghn7wpbcvg4pj2wwwjecyz253ewoycnmi22elm.py')
DEBUG 07-03 03:55:42 [backends.py:153] TOTAL LOADING TIME: 0.051901 s
DEBUG 07-03 03:55:42 [backends.py:125] Loading time: 0.053910	 Directly load the 22-th graph for shape None from inductor via handle ('fsx6a7bzzfqjfn5ohxknaxvfze4hskr5o2nk4pj5gennyl7u2v3q', '/root/.cache/vllm/torch_compile_cache/56d5ca6229/rank_0_0/inductor_cache/gx/cgxorvo2ds2xu4ghn7wpbcvg4pj2wwwjecyz253ewoycnmi22elm.py')
DEBUG 07-03 03:55:42 [backends.py:153] TOTAL LOADING TIME: 0.054019 s
DEBUG 07-03 03:55:43 [backends.py:125] Loading time: 0.054061	 Directly load the 23-th graph for shape None from inductor via handle ('fsx6a7bzzfqjfn5ohxknaxvfze4hskr5o2nk4pj5gennyl7u2v3q', '/root/.cache/vllm/torch_compile_cache/56d5ca6229/rank_0_0/inductor_cache/gx/cgxorvo2ds2xu4ghn7wpbcvg4pj2wwwjecyz253ewoycnmi22elm.py')
DEBUG 07-03 03:55:43 [backends.py:153] TOTAL LOADING TIME: 0.054175 s
DEBUG 07-03 03:55:43 [backends.py:125] Loading time: 0.050042	 Directly load the 24-th graph for shape None from inductor via handle ('fsx6a7bzzfqjfn5ohxknaxvfze4hskr5o2nk4pj5gennyl7u2v3q', '/root/.cache/vllm/torch_compile_cache/56d5ca6229/rank_0_0/inductor_cache/gx/cgxorvo2ds2xu4ghn7wpbcvg4pj2wwwjecyz253ewoycnmi22elm.py')
DEBUG 07-03 03:55:43 [backends.py:153] TOTAL LOADING TIME: 0.050150 s
DEBUG 07-03 03:55:43 [backends.py:125] Loading time: 0.054337	 Directly load the 25-th graph for shape None from inductor via handle ('fsx6a7bzzfqjfn5ohxknaxvfze4hskr5o2nk4pj5gennyl7u2v3q', '/root/.cache/vllm/torch_compile_cache/56d5ca6229/rank_0_0/inductor_cache/gx/cgxorvo2ds2xu4ghn7wpbcvg4pj2wwwjecyz253ewoycnmi22elm.py')
DEBUG 07-03 03:55:43 [backends.py:153] TOTAL LOADING TIME: 0.054475 s
DEBUG 07-03 03:55:43 [backends.py:125] Loading time: 0.050820	 Directly load the 26-th graph for shape None from inductor via handle ('fsx6a7bzzfqjfn5ohxknaxvfze4hskr5o2nk4pj5gennyl7u2v3q', '/root/.cache/vllm/torch_compile_cache/56d5ca6229/rank_0_0/inductor_cache/gx/cgxorvo2ds2xu4ghn7wpbcvg4pj2wwwjecyz253ewoycnmi22elm.py')
DEBUG 07-03 03:55:43 [backends.py:153] TOTAL LOADING TIME: 0.050925 s
DEBUG 07-03 03:55:43 [backends.py:125] Loading time: 0.055598	 Directly load the 27-th graph for shape None from inductor via handle ('fsx6a7bzzfqjfn5ohxknaxvfze4hskr5o2nk4pj5gennyl7u2v3q', '/root/.cache/vllm/torch_compile_cache/56d5ca6229/rank_0_0/inductor_cache/gx/cgxorvo2ds2xu4ghn7wpbcvg4pj2wwwjecyz253ewoycnmi22elm.py')
DEBUG 07-03 03:55:43 [backends.py:153] TOTAL LOADING TIME: 0.055701 s
DEBUG 07-03 03:55:43 [backends.py:125] Loading time: 0.057689	 Directly load the 28-th graph for shape None from inductor via handle ('fsx6a7bzzfqjfn5ohxknaxvfze4hskr5o2nk4pj5gennyl7u2v3q', '/root/.cache/vllm/torch_compile_cache/56d5ca6229/rank_0_0/inductor_cache/gx/cgxorvo2ds2xu4ghn7wpbcvg4pj2wwwjecyz253ewoycnmi22elm.py')
DEBUG 07-03 03:55:43 [backends.py:153] TOTAL LOADING TIME: 0.057794 s
DEBUG 07-03 03:55:44 [backends.py:125] Loading time: 0.049889	 Directly load the 29-th graph for shape None from inductor via handle ('fsx6a7bzzfqjfn5ohxknaxvfze4hskr5o2nk4pj5gennyl7u2v3q', '/root/.cache/vllm/torch_compile_cache/56d5ca6229/rank_0_0/inductor_cache/gx/cgxorvo2ds2xu4ghn7wpbcvg4pj2wwwjecyz253ewoycnmi22elm.py')
DEBUG 07-03 03:55:44 [backends.py:153] TOTAL LOADING TIME: 0.050011 s
DEBUG 07-03 03:55:44 [backends.py:125] Loading time: 0.056780	 Directly load the 30-th graph for shape None from inductor via handle ('fsx6a7bzzfqjfn5ohxknaxvfze4hskr5o2nk4pj5gennyl7u2v3q', '/root/.cache/vllm/torch_compile_cache/56d5ca6229/rank_0_0/inductor_cache/gx/cgxorvo2ds2xu4ghn7wpbcvg4pj2wwwjecyz253ewoycnmi22elm.py')
DEBUG 07-03 03:55:44 [backends.py:153] TOTAL LOADING TIME: 0.056926 s
DEBUG 07-03 03:55:44 [backends.py:125] Loading time: 0.052563	 Directly load the 31-th graph for shape None from inductor via handle ('fsx6a7bzzfqjfn5ohxknaxvfze4hskr5o2nk4pj5gennyl7u2v3q', '/root/.cache/vllm/torch_compile_cache/56d5ca6229/rank_0_0/inductor_cache/gx/cgxorvo2ds2xu4ghn7wpbcvg4pj2wwwjecyz253ewoycnmi22elm.py')
DEBUG 07-03 03:55:44 [backends.py:153] TOTAL LOADING TIME: 0.052709 s
DEBUG 07-03 03:55:44 [backends.py:125] Loading time: 0.025580	 Directly load the 32-th graph for shape None from inductor via handle ('fumk6h2q3gjx2eii2ho4i4pz445zknwgo6ouxiy6sike4gaue66y', '/root/.cache/vllm/torch_compile_cache/56d5ca6229/rank_0_0/inductor_cache/6w/c6wl4mx464mijy2qndgjajph6voailsmoszj76undq4zjlbjaxup.py')
DEBUG 07-03 03:55:44 [backends.py:153] TOTAL LOADING TIME: 0.025726 s
INFO 07-03 03:55:44 [backends.py:160] Directly load the compiled graph(s) for shape None from the cache, took 5.153 s
INFO 07-03 03:55:45 [monitor.py:34] torch.compile takes 6.70 s in total
DEBUG 07-03 03:55:46 [gpu_worker.py:226] Initial free memory: 43.89 GiB, free memory: 28.76 GiB, requested GPU memory: 39.88 GiB
DEBUG 07-03 03:55:46 [gpu_worker.py:231] Memory profiling takes 13.63 seconds. Total non KV cache memory: 15.53GiB; torch peak memory increase: 0.46GiB; non-torch forward increase memory: 0.08GiB; weights memory: 14.99GiB.
INFO 07-03 03:55:46 [gpu_worker.py:232] Available KV cache memory: 24.35 GiB
INFO 07-03 03:55:46 [kv_cache_utils.py:716] GPU KV cache size: 199,472 tokens
INFO 07-03 03:55:46 [kv_cache_utils.py:720] Maximum concurrency for 131,072 tokens per request: 1.52x
WARNING 07-03 03:55:46 [utils.py:101] Unable to detect current VLLM config. Defaulting to NHD kv cache layout.
DEBUG 07-03 03:55:46 [config.py:4806] enabled custom ops: Counter()
DEBUG 07-03 03:55:46 [config.py:4808] disabled custom ops: Counter({'rms_norm': 65, 'silu_and_mul': 32, 'rotary_embedding': 1})
Capturing CUDA graphs:   0%|                                                                                              | 0/67 [00:00<?, ?it/s]DEBUG 07-03 03:55:46 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 512
DEBUG 07-03 03:55:46 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 512
Capturing CUDA graphs:   1%|█▎                                                                                    | 1/67 [00:00<00:17,  3.78it/s]DEBUG 07-03 03:55:46 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 504
DEBUG 07-03 03:55:46 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 504
Capturing CUDA graphs:   3%|██▌                                                                                   | 2/67 [00:00<00:17,  3.76it/s]DEBUG 07-03 03:55:46 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 496
DEBUG 07-03 03:55:46 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 496
Capturing CUDA graphs:   4%|███▊                                                                                  | 3/67 [00:00<00:17,  3.65it/s]DEBUG 07-03 03:55:47 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 488
DEBUG 07-03 03:55:47 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 488
Capturing CUDA graphs:   6%|█████▏                                                                                | 4/67 [00:01<00:16,  3.72it/s]DEBUG 07-03 03:55:47 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 480
DEBUG 07-03 03:55:47 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 480
Capturing CUDA graphs:   7%|██████▍                                                                               | 5/67 [00:01<00:16,  3.73it/s]DEBUG 07-03 03:55:47 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 472
DEBUG 07-03 03:55:47 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 472
Capturing CUDA graphs:   9%|███████▋                                                                              | 6/67 [00:01<00:16,  3.75it/s]DEBUG 07-03 03:55:47 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 464
DEBUG 07-03 03:55:47 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 464
Capturing CUDA graphs:  10%|████████▉                                                                             | 7/67 [00:01<00:15,  3.83it/s]DEBUG 07-03 03:55:48 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 456
DEBUG 07-03 03:55:48 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 456
DEBUG 07-03 03:55:48 [utils.py:485] Waiting for 1 local, 0 remote core engine proc(s) to start.
Capturing CUDA graphs:  12%|██████████▎                                                                           | 8/67 [00:02<00:15,  3.90it/s]DEBUG 07-03 03:55:48 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 448
DEBUG 07-03 03:55:48 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 448
Capturing CUDA graphs:  13%|███████████▌                                                                          | 9/67 [00:02<00:14,  3.95it/s]DEBUG 07-03 03:55:48 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 440
DEBUG 07-03 03:55:48 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 440
Capturing CUDA graphs:  15%|████████████▋                                                                        | 10/67 [00:02<00:14,  3.88it/s]DEBUG 07-03 03:55:48 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 432
DEBUG 07-03 03:55:48 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 432
Capturing CUDA graphs:  16%|█████████████▉                                                                       | 11/67 [00:02<00:14,  3.84it/s]DEBUG 07-03 03:55:49 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 424
DEBUG 07-03 03:55:49 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 424
Capturing CUDA graphs:  18%|███████████████▏                                                                     | 12/67 [00:03<00:14,  3.89it/s]DEBUG 07-03 03:55:49 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 416
DEBUG 07-03 03:55:49 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 416
Capturing CUDA graphs:  19%|████████████████▍                                                                    | 13/67 [00:03<00:13,  3.93it/s]DEBUG 07-03 03:55:49 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 408
DEBUG 07-03 03:55:49 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 408
Capturing CUDA graphs:  21%|█████████████████▊                                                                   | 14/67 [00:03<00:13,  3.85it/s]DEBUG 07-03 03:55:49 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 400
DEBUG 07-03 03:55:49 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 400
Capturing CUDA graphs:  22%|███████████████████                                                                  | 15/67 [00:03<00:13,  3.75it/s]DEBUG 07-03 03:55:50 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 392
DEBUG 07-03 03:55:50 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 392
Capturing CUDA graphs:  24%|████████████████████▎                                                                | 16/67 [00:04<00:13,  3.82it/s]DEBUG 07-03 03:55:50 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 384
DEBUG 07-03 03:55:50 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 384
Capturing CUDA graphs:  25%|█████████████████████▌                                                               | 17/67 [00:04<00:12,  3.89it/s]DEBUG 07-03 03:55:50 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 376
DEBUG 07-03 03:55:50 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 376
Capturing CUDA graphs:  27%|██████████████████████▊                                                              | 18/67 [00:04<00:12,  3.96it/s]DEBUG 07-03 03:55:50 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 368
DEBUG 07-03 03:55:50 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 368
Capturing CUDA graphs:  28%|████████████████████████                                                             | 19/67 [00:04<00:12,  3.85it/s]DEBUG 07-03 03:55:51 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 360
DEBUG 07-03 03:55:51 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 360
Capturing CUDA graphs:  30%|█████████████████████████▎                                                           | 20/67 [00:05<00:12,  3.83it/s]DEBUG 07-03 03:55:51 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 352
DEBUG 07-03 03:55:51 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 352
Capturing CUDA graphs:  31%|██████████████████████████▋                                                          | 21/67 [00:05<00:11,  3.84it/s]DEBUG 07-03 03:55:51 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 344
DEBUG 07-03 03:55:51 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 344
Capturing CUDA graphs:  33%|███████████████████████████▉                                                         | 22/67 [00:05<00:11,  3.83it/s]DEBUG 07-03 03:55:51 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 336
DEBUG 07-03 03:55:52 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 336
Capturing CUDA graphs:  34%|█████████████████████████████▏                                                       | 23/67 [00:06<00:11,  3.78it/s]DEBUG 07-03 03:55:52 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 328
DEBUG 07-03 03:55:52 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 328
Capturing CUDA graphs:  36%|██████████████████████████████▍                                                      | 24/67 [00:06<00:11,  3.65it/s]DEBUG 07-03 03:55:52 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 320
DEBUG 07-03 03:55:52 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 320
Capturing CUDA graphs:  37%|███████████████████████████████▋                                                     | 25/67 [00:06<00:11,  3.74it/s]DEBUG 07-03 03:55:52 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 312
DEBUG 07-03 03:55:52 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 312
Capturing CUDA graphs:  39%|████████████████████████████████▉                                                    | 26/67 [00:06<00:10,  3.83it/s]DEBUG 07-03 03:55:53 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 304
DEBUG 07-03 03:55:53 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 304
Capturing CUDA graphs:  40%|██████████████████████████████████▎                                                  | 27/67 [00:07<00:10,  3.80it/s]DEBUG 07-03 03:55:53 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 296
DEBUG 07-03 03:55:53 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 296
Capturing CUDA graphs:  42%|███████████████████████████████████▌                                                 | 28/67 [00:07<00:10,  3.77it/s]DEBUG 07-03 03:55:53 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 288
DEBUG 07-03 03:55:53 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 288
Capturing CUDA graphs:  43%|████████████████████████████████████▊                                                | 29/67 [00:07<00:09,  3.83it/s]DEBUG 07-03 03:55:53 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 280
DEBUG 07-03 03:55:53 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 280
Capturing CUDA graphs:  45%|██████████████████████████████████████                                               | 30/67 [00:07<00:09,  3.88it/s]DEBUG 07-03 03:55:54 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 272
DEBUG 07-03 03:55:54 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 272
Capturing CUDA graphs:  46%|███████████████████████████████████████▎                                             | 31/67 [00:08<00:09,  3.90it/s]DEBUG 07-03 03:55:54 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 264
DEBUG 07-03 03:55:54 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 264
Capturing CUDA graphs:  48%|████████████████████████████████████████▌                                            | 32/67 [00:08<00:08,  3.97it/s]DEBUG 07-03 03:55:54 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 256
DEBUG 07-03 03:55:54 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 256
Capturing CUDA graphs:  49%|█████████████████████████████████████████▊                                           | 33/67 [00:08<00:08,  3.88it/s]DEBUG 07-03 03:55:54 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 248
DEBUG 07-03 03:55:54 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 248
Capturing CUDA graphs:  51%|███████████████████████████████████████████▏                                         | 34/67 [00:08<00:08,  3.86it/s]DEBUG 07-03 03:55:55 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 240
DEBUG 07-03 03:55:55 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 240
Capturing CUDA graphs:  52%|████████████████████████████████████████████▍                                        | 35/67 [00:09<00:08,  3.77it/s]DEBUG 07-03 03:55:55 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 232
DEBUG 07-03 03:55:55 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 232
Capturing CUDA graphs:  54%|█████████████████████████████████████████████▋                                       | 36/67 [00:09<00:07,  3.88it/s]DEBUG 07-03 03:55:55 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 224
DEBUG 07-03 03:55:55 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 224
Capturing CUDA graphs:  55%|██████████████████████████████████████████████▉                                      | 37/67 [00:09<00:07,  3.96it/s]DEBUG 07-03 03:55:55 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 216
DEBUG 07-03 03:55:55 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 216
Capturing CUDA graphs:  57%|████████████████████████████████████████████████▏                                    | 38/67 [00:09<00:07,  3.94it/s]DEBUG 07-03 03:55:56 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 208
DEBUG 07-03 03:55:56 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 208
Capturing CUDA graphs:  58%|█████████████████████████████████████████████████▍                                   | 39/67 [00:10<00:07,  3.95it/s]DEBUG 07-03 03:55:56 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 200
DEBUG 07-03 03:55:56 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 200
Capturing CUDA graphs:  60%|██████████████████████████████████████████████████▋                                  | 40/67 [00:10<00:06,  3.97it/s]DEBUG 07-03 03:55:56 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 192
DEBUG 07-03 03:55:56 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 192
Capturing CUDA graphs:  61%|████████████████████████████████████████████████████                                 | 41/67 [00:10<00:06,  4.00it/s]DEBUG 07-03 03:55:56 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 184
DEBUG 07-03 03:55:56 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 184
Capturing CUDA graphs:  63%|█████████████████████████████████████████████████████▎                               | 42/67 [00:10<00:06,  3.85it/s]DEBUG 07-03 03:55:57 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 176
DEBUG 07-03 03:55:57 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 176
Capturing CUDA graphs:  64%|██████████████████████████████████████████████████████▌                              | 43/67 [00:11<00:06,  3.80it/s]DEBUG 07-03 03:55:57 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 168
DEBUG 07-03 03:55:57 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 168
Capturing CUDA graphs:  66%|███████████████████████████████████████████████████████▊                             | 44/67 [00:11<00:05,  3.87it/s]DEBUG 07-03 03:55:57 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 160
DEBUG 07-03 03:55:57 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 160
Capturing CUDA graphs:  67%|█████████████████████████████████████████████████████████                            | 45/67 [00:11<00:05,  3.95it/s]DEBUG 07-03 03:55:57 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 152
DEBUG 07-03 03:55:57 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 152
Capturing CUDA graphs:  69%|██████████████████████████████████████████████████████████▎                          | 46/67 [00:11<00:05,  3.98it/s]DEBUG 07-03 03:55:58 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 144
DEBUG 07-03 03:55:58 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 144
DEBUG 07-03 03:55:58 [utils.py:485] Waiting for 1 local, 0 remote core engine proc(s) to start.
Capturing CUDA graphs:  70%|███████████████████████████████████████████████████████████▋                         | 47/67 [00:12<00:05,  3.96it/s]DEBUG 07-03 03:55:58 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 136
DEBUG 07-03 03:55:58 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 136
Capturing CUDA graphs:  72%|████████████████████████████████████████████████████████████▉                        | 48/67 [00:12<00:04,  3.98it/s]DEBUG 07-03 03:55:58 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 128
DEBUG 07-03 03:55:58 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 128
Capturing CUDA graphs:  73%|██████████████████████████████████████████████████████████████▏                      | 49/67 [00:12<00:04,  4.04it/s]DEBUG 07-03 03:55:58 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 120
DEBUG 07-03 03:55:58 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 120
Capturing CUDA graphs:  75%|███████████████████████████████████████████████████████████████▍                     | 50/67 [00:12<00:04,  4.04it/s]DEBUG 07-03 03:55:59 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 112
DEBUG 07-03 03:55:59 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 112
Capturing CUDA graphs:  76%|████████████████████████████████████████████████████████████████▋                    | 51/67 [00:13<00:03,  4.08it/s]DEBUG 07-03 03:55:59 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 104
DEBUG 07-03 03:55:59 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 104
Capturing CUDA graphs:  78%|█████████████████████████████████████████████████████████████████▉                   | 52/67 [00:13<00:03,  3.93it/s]DEBUG 07-03 03:55:59 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 96
DEBUG 07-03 03:55:59 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 96
Capturing CUDA graphs:  79%|███████████████████████████████████████████████████████████████████▏                 | 53/67 [00:13<00:03,  3.94it/s]DEBUG 07-03 03:55:59 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 88
DEBUG 07-03 03:55:59 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 88
Capturing CUDA graphs:  81%|████████████████████████████████████████████████████████████████████▌                | 54/67 [00:13<00:03,  3.95it/s]DEBUG 07-03 03:56:00 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 80
DEBUG 07-03 03:56:00 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 80
Capturing CUDA graphs:  82%|█████████████████████████████████████████████████████████████████████▊               | 55/67 [00:14<00:03,  3.90it/s]DEBUG 07-03 03:56:00 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 72
DEBUG 07-03 03:56:00 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 72
Capturing CUDA graphs:  84%|███████████████████████████████████████████████████████████████████████              | 56/67 [00:14<00:02,  3.90it/s]DEBUG 07-03 03:56:00 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 64
DEBUG 07-03 03:56:00 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 64
Capturing CUDA graphs:  85%|████████████████████████████████████████████████████████████████████████▎            | 57/67 [00:14<00:02,  3.93it/s]DEBUG 07-03 03:56:00 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 56
DEBUG 07-03 03:56:00 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 56
Capturing CUDA graphs:  87%|█████████████████████████████████████████████████████████████████████████▌           | 58/67 [00:14<00:02,  3.91it/s]DEBUG 07-03 03:56:01 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 48
DEBUG 07-03 03:56:01 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 48
Capturing CUDA graphs:  88%|██████████████████████████████████████████████████████████████████████████▊          | 59/67 [00:15<00:02,  3.88it/s]DEBUG 07-03 03:56:01 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 40
DEBUG 07-03 03:56:01 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 40
Capturing CUDA graphs:  90%|████████████████████████████████████████████████████████████████████████████         | 60/67 [00:15<00:01,  3.73it/s]DEBUG 07-03 03:56:01 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 32
DEBUG 07-03 03:56:01 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 32
Capturing CUDA graphs:  91%|█████████████████████████████████████████████████████████████████████████████▍       | 61/67 [00:15<00:01,  3.66it/s]DEBUG 07-03 03:56:02 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 24
DEBUG 07-03 03:56:02 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 24
Capturing CUDA graphs:  93%|██████████████████████████████████████████████████████████████████████████████▋      | 62/67 [00:16<00:01,  3.74it/s]DEBUG 07-03 03:56:02 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 16
DEBUG 07-03 03:56:02 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 16
Capturing CUDA graphs:  94%|███████████████████████████████████████████████████████████████████████████████▉     | 63/67 [00:16<00:01,  3.88it/s]DEBUG 07-03 03:56:02 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 8
DEBUG 07-03 03:56:02 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 8
Capturing CUDA graphs:  96%|█████████████████████████████████████████████████████████████████████████████████▏   | 64/67 [00:16<00:00,  3.91it/s]DEBUG 07-03 03:56:02 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 4
DEBUG 07-03 03:56:02 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 4
Capturing CUDA graphs:  97%|██████████████████████████████████████████████████████████████████████████████████▍  | 65/67 [00:16<00:00,  3.93it/s]DEBUG 07-03 03:56:03 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 2
DEBUG 07-03 03:56:03 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 2
Capturing CUDA graphs:  99%|███████████████████████████████████████████████████████████████████████████████████▋ | 66/67 [00:17<00:00,  3.90it/s]DEBUG 07-03 03:56:03 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 1
DEBUG 07-03 03:56:03 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 1
Capturing CUDA graphs: 100%|█████████████████████████████████████████████████████████████████████████████████████| 67/67 [00:17<00:00,  3.87it/s]
INFO 07-03 03:56:03 [gpu_model_runner.py:2279] Graph capturing finished in 17 secs, took 0.54 GiB
INFO 07-03 03:56:03 [core.py:172] init engine (profile, create kv cache, warmup model) took 31.36 seconds
DEBUG 07-03 03:56:04 [utils.py:547] READY from local core engine process 0.
DEBUG 07-03 03:56:04 [core.py:547] EngineCore waiting for work.
INFO 07-03 03:56:04 [loggers.py:137] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 12467
DEBUG 07-03 03:56:04 [core.py:547] EngineCore waiting for work.
WARNING 07-03 03:56:04 [config.py:1394] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
INFO 07-03 03:56:04 [serving_chat.py:121] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
INFO 07-03 03:56:04 [serving_completion.py:68] Using default completion sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
INFO 07-03 03:56:04 [api_server.py:1450] Starting vLLM API server 0 on http://0.0.0.0:8000
INFO 07-03 03:56:04 [launcher.py:29] Available routes are:
INFO 07-03 03:56:04 [launcher.py:37] Route: /openapi.json, Methods: HEAD, GET
INFO 07-03 03:56:04 [launcher.py:37] Route: /docs, Methods: HEAD, GET
INFO 07-03 03:56:04 [launcher.py:37] Route: /docs/oauth2-redirect, Methods: HEAD, GET
INFO 07-03 03:56:04 [launcher.py:37] Route: /redoc, Methods: HEAD, GET
INFO 07-03 03:56:04 [launcher.py:37] Route: /health, Methods: GET
INFO 07-03 03:56:04 [launcher.py:37] Route: /load, Methods: GET
INFO 07-03 03:56:04 [launcher.py:37] Route: /ping, Methods: POST
INFO 07-03 03:56:04 [launcher.py:37] Route: /ping, Methods: GET
INFO 07-03 03:56:04 [launcher.py:37] Route: /tokenize, Methods: POST
INFO 07-03 03:56:04 [launcher.py:37] Route: /detokenize, Methods: POST
INFO 07-03 03:56:04 [launcher.py:37] Route: /v1/models, Methods: GET
INFO 07-03 03:56:04 [launcher.py:37] Route: /version, Methods: GET
INFO 07-03 03:56:04 [launcher.py:37] Route: /v1/chat/completions, Methods: POST
INFO 07-03 03:56:04 [launcher.py:37] Route: /v1/completions, Methods: POST
INFO 07-03 03:56:04 [launcher.py:37] Route: /v1/embeddings, Methods: POST
INFO 07-03 03:56:04 [launcher.py:37] Route: /pooling, Methods: POST
INFO 07-03 03:56:04 [launcher.py:37] Route: /classify, Methods: POST
INFO 07-03 03:56:04 [launcher.py:37] Route: /score, Methods: POST
INFO 07-03 03:56:04 [launcher.py:37] Route: /v1/score, Methods: POST
INFO 07-03 03:56:04 [launcher.py:37] Route: /v1/audio/transcriptions, Methods: POST
INFO 07-03 03:56:04 [launcher.py:37] Route: /v1/audio/translations, Methods: POST
INFO 07-03 03:56:04 [launcher.py:37] Route: /rerank, Methods: POST
INFO 07-03 03:56:04 [launcher.py:37] Route: /v1/rerank, Methods: POST
INFO 07-03 03:56:04 [launcher.py:37] Route: /v2/rerank, Methods: POST
INFO 07-03 03:56:04 [launcher.py:37] Route: /invocations, Methods: POST
INFO 07-03 03:56:04 [launcher.py:37] Route: /metrics, Methods: GET
INFO:     Started server process [5788]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
DEBUG 07-03 03:56:14 [loggers.py:118] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
^CDEBUG 07-03 03:56:24 [core.py:515] EngineCore exiting.
DEBUG 07-03 03:56:24 [launcher.py:77] port 8000 is used by process psutil.Process(pid=5788, name='vllm', status='running', started='03:54:47') launched with command:
DEBUG 07-03 03:56:24 [launcher.py:77] /usr/bin/python3.12 /usr/local/bin/vllm serve meta-llama/Llama-3.1-8B-Instruct
INFO 07-03 03:56:24 [launcher.py:80] Shutting down FastAPI HTTP server.
[rank0]:[W703 03:56:24.216292366 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
INFO:     Shutting down
DEBUG 07-03 03:56:25 [loggers.py:118] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO:     Waiting for application shutdown.
INFO:     Application shutdown complete.
